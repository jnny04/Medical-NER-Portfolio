{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Medical Named Entity Recognition (NER)**\n",
    "\n",
    "Membandingkan akurasi vs efisiensi komputasi. Disini saya membandingkan model \"berat\" (seperti BERT) dengan model \"ringan\" (seperti scispacy/CNN) untuk membuktikan mana yang paling cocok diterapkan di rumah sakit di negara berkembang (Indonesia) yang servernya terbatas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Eksperimen 1**\n",
    "\n",
    "**Dataset:** BC5CDR (BioCreative V CDR Task)\n",
    "\n",
    "**Tujuan:** Membangun model AI untuk mendeteksi entitas Penyakit (Disease) dan Zat Kimia (Chemical) secara otomatis dari teks medis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tahap 1: Persiapan Data (Data Preparation)**\n",
    "Langkah ini bertujuan untuk memuat data mentah (JSON) dan memverifikasi strukturnya sebelum masuk ke pelatihan model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Library Utama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Perangkat yang digunakan: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Import Library Utama ---\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Library Transformers (Hugging Face)\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "from torch.optim import AdamW \n",
    "from tqdm.auto import tqdm # Untuk loading bar\n",
    "\n",
    "# Mengatur tampilan pandas agar tabel tidak terpotong\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Cek apakah menggunakan GPU (NVIDIA) atau CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"‚öôÔ∏è Perangkat yang digunakan: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fungsi untuk Membaca File JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fungsi untuk Membaca File JSON ---\n",
    "def load_json_file(file_path):\n",
    "    data = []\n",
    "    try:\n",
    "        # Coba baca format JSON Lines (per baris)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "        print(f\"‚úÖ Berhasil memuat: {os.path.basename(file_path)} ({len(data)} kalimat)\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gagal memuat {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Tentukan lokasi folder dataset\n",
    "dataset_path = 'bc5cdr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memuat Dataset Train, Validasi, dan Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Sedang membaca dataset...\n",
      "‚úÖ Berhasil memuat: train.json (5228 kalimat)\n",
      "‚úÖ Berhasil memuat: valid.json (5330 kalimat)\n",
      "‚úÖ Berhasil memuat: test.json (5865 kalimat)\n"
     ]
    }
   ],
   "source": [
    "# --- Memuat Data Training, Validasi, dan Testing ---\n",
    "\n",
    "print(\"üìÇ Sedang membaca dataset...\")\n",
    "train_data = load_json_file(os.path.join(dataset_path, 'train.json'))\n",
    "valid_data = load_json_file(os.path.join(dataset_path, 'valid.json'))\n",
    "test_data  = load_json_file(os.path.join(dataset_path, 'test.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Konfigurasi Mapping Label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Label Map berhasil dimuat.\n",
      "Daftar Kategori:\n",
      "  0: O\n",
      "  1: B-Chemical\n",
      "  2: B-Disease\n",
      "  3: I-Disease\n",
      "  4: I-Chemical\n"
     ]
    }
   ],
   "source": [
    "# --- Memuat Label & Membuat Mapping ---\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(dataset_path, 'label.json'), 'r') as f:\n",
    "        label_map = json.load(f) # Isinya {'B-Chemical': 1, ...}\n",
    "\n",
    "    # Kita butuh kebalikannya: Angka -> Nama Label (untuk manusia membaca)\n",
    "    id2label = {v: k for k, v in label_map.items()}\n",
    "    label2id = label_map\n",
    "\n",
    "    print(\"\\n‚úÖ Label Map berhasil dimuat.\")\n",
    "    print(\"Daftar Kategori:\")\n",
    "    for id_angka, nama in id2label.items():\n",
    "        print(f\"  {id_angka}: {nama}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå label.json tidak ditemukan! Pastikan file ada di folder 'bc5cdr'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualisasi Sampel Data Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags_readable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Naloxone, reverses, the, antihypertensive, effect, of, clonidine, .]</td>\n",
       "      <td>[B-Chemical, O, O, O, O, O, B-Chemical, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[In, unanesthetized, ,, spontaneously, hypertensive, rats, the, decrease, in, blood, pressure, and, heart, rate, produced, by, intravenous, clonidine, ,, 5, to, 20, micrograms, /, kg, ,, was, inhibited, or, reversed, by, nalozone, ,, 0, .]</td>\n",
       "      <td>[O, O, O, O, B-Disease, O, O, O, O, O, O, O, O, O, O, O, O, B-Chemical, O, O, O, O, O, O, O, O, O, O, O, O, O, B-Chemical, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, to, 2, mg, /, kg, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, hypotensive, effect, of, 100, mg, /, kg, alpha-methyldopa, was, also, partially, reversed, by, naloxone, .]</td>\n",
       "      <td>[O, B-Disease, O, O, O, O, O, O, B-Chemical, O, O, O, O, O, B-Chemical, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Naloxone, alone, did, not, affect, either, blood, pressure, or, heart, rate, .]</td>\n",
       "      <td>[B-Chemical, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                            tokens  \\\n",
       "0                                                                                                                                                                            [Naloxone, reverses, the, antihypertensive, effect, of, clonidine, .]   \n",
       "1  [In, unanesthetized, ,, spontaneously, hypertensive, rats, the, decrease, in, blood, pressure, and, heart, rate, produced, by, intravenous, clonidine, ,, 5, to, 20, micrograms, /, kg, ,, was, inhibited, or, reversed, by, nalozone, ,, 0, .]   \n",
       "2                                                                                                                                                                                                                         [2, to, 2, mg, /, kg, .]   \n",
       "3                                                                                                                                [The, hypotensive, effect, of, 100, mg, /, kg, alpha-methyldopa, was, also, partially, reversed, by, naloxone, .]   \n",
       "4                                                                                                                                                                 [Naloxone, alone, did, not, affect, either, blood, pressure, or, heart, rate, .]   \n",
       "\n",
       "                                                                                                                         tags_readable  \n",
       "0                                                                                           [B-Chemical, O, O, O, O, O, B-Chemical, O]  \n",
       "1  [O, O, O, O, B-Disease, O, O, O, O, O, O, O, O, O, O, O, O, B-Chemical, O, O, O, O, O, O, O, O, O, O, O, O, O, B-Chemical, O, O, O]  \n",
       "2                                                                                                                [O, O, O, O, O, O, O]  \n",
       "3                                                           [O, B-Disease, O, O, O, O, O, O, B-Chemical, O, O, O, O, O, B-Chemical, O]  \n",
       "4                                                                                        [B-Chemical, O, O, O, O, O, O, O, O, O, O, O]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Konversi ke Pandas DataFrame untuk kemudahan visualisasi\n",
    "df_train = pd.DataFrame(train_data)\n",
    "\n",
    "# Fungsi untuk menerjemahkan list angka tag menjadi list nama label\n",
    "def angka_ke_label(tag_list):\n",
    "    return [id2label[x] for x in tag_list]\n",
    "\n",
    "# Kita ambil 5 sampel untuk dicek\n",
    "df_sample = df_train.head(5).copy()\n",
    "\n",
    "# Buat kolom baru yang isinya nama label (bukan angka) supaya mudah dibaca manusia\n",
    "df_sample['tags_readable'] = df_sample['tags'].apply(angka_ke_label)\n",
    "\n",
    "# Tampilkan kolom tokens (katanya) dan tags_readable (labelnya)\n",
    "display(df_sample[['tokens', 'tags_readable']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tahap 2: Tokenisasi dan Penyelarasan Label (Label Alignment)**\n",
    "Kita akan menggunakan **BERT Tokenizer** (`bert-base-cased`).\n",
    "Model ini dipilih karena \"Cased\" (memperhatikan huruf besar/kecil) sangat penting untuk mendeteksi nama obat atau penyakit (contoh: \"Vitamin D\" vs \"d\").\n",
    "\n",
    "Tantangan utama di sini adalah **Sub-word Tokenization**.\n",
    "Jika kata `Hydroxychloroquine` (1 kata) dipecah menjadi `['Hy', '##dro', '##xy', ...]` (banyak token), kita harus memastikan label `B-Chemical` hanya menempel pada token pertama (`Hy`), sedangkan pecahan sisanya kita beri label `-100` (agar diabaikan saat perhitungan error/loss nanti)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memuat BERT Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Sedang mengunduh vocabulary dmis-lab/biobert-v1.1...\n",
      "‚úÖ Tokenizer BioBERT siap!\n"
     ]
    }
   ],
   "source": [
    "# --- Memuat Tokenizer BioBERT ---\n",
    "\n",
    "# Kita gunakan model yang sama untuk tokenizer\n",
    "MODEL_CHECKPOINT = \"dmis-lab/biobert-v1.1\"\n",
    "\n",
    "print(f\"‚è≥ Sedang mengunduh vocabulary {MODEL_CHECKPOINT}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "print(\"‚úÖ Tokenizer BioBERT siap!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer berhasil dimuat!\n",
      "Contoh pemecahan kata 'Hydrochlorothiazide':\n",
      "['H', '##ydro', '##ch', '##lor', '##oth', '##ia', '##zi', '##de']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Kita gunakan \"bert-base-cased\"\n",
    "# Model ini standar industri untuk tugas NER dasar\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "print(\"‚úÖ Tokenizer berhasil dimuat!\")\n",
    "\n",
    "# Tes pada satu kata yang sulit\n",
    "contoh_kata = \"Hydrochlorothiazide\"\n",
    "hasil_token = tokenizer.tokenize(contoh_kata)\n",
    "print(f\"Contoh pemecahan kata '{contoh_kata}':\")\n",
    "print(hasil_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fungsi Tokenisasi & Alignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer siap digunakan.\n"
     ]
    }
   ],
   "source": [
    "# --- Memuat Tokenizer & Fungsi Alignment ---\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Kita pakai model standar yang AMAN (mendukung SafeTensors)\n",
    "MODEL_CHECKPOINT = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Fungsi untuk memotong kata dan menyamakan label\n",
    "def tokenize_and_align_labels(dataset_list):\n",
    "    all_tokens = [item[\"tokens\"] for item in dataset_list]\n",
    "    all_tags   = [item[\"tags\"] for item in dataset_list]\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        all_tokens, \n",
    "        truncation=True, \n",
    "        is_split_into_words=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label_asli in enumerate(all_tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        prev_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == prev_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                try:\n",
    "                    label_ids.append(label_asli[word_idx])\n",
    "                except:\n",
    "                    label_ids.append(-100)\n",
    "            prev_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(\"‚úÖ Tokenizer siap digunakan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proses Data dan DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Sedang memproses data...\n",
      "‚úÖ Data siap dilatih!\n"
     ]
    }
   ],
   "source": [
    "# --- Proses Data & Buat DataLoader ---\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Proses Data\n",
    "print(\"‚è≥ Sedang memproses data...\")\n",
    "tokenized_train = tokenize_and_align_labels(train_data)\n",
    "tokenized_valid = tokenize_and_align_labels(valid_data)\n",
    "\n",
    "# 2. Buat Wadah Dataset\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings): self.encodings = encodings\n",
    "    def __getitem__(self, i): return {k: torch.tensor(v[i]) for k, v in self.encodings.items()}\n",
    "    def __len__(self): return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# 3. Buat Pengirim Data (Loader)\n",
    "train_loader = DataLoader(NERDataset(tokenized_train), batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(NERDataset(tokenized_valid), batch_size=8)\n",
    "\n",
    "print(\"‚úÖ Data siap dilatih!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cek Hasil Tokenisasi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['Naloxone', 'reverses', 'the', 'antihypertensive', 'effect', 'of', 'clonidine', '.']\n",
      "------------------------------\n",
      "TOKEN           LABEL ID   ARTI LABEL\n",
      "[CLS]           -100       IGNORE\n",
      "Na              1          B-Chemical\n",
      "##lo            -100       IGNORE\n",
      "##xon           -100       IGNORE\n",
      "##e             -100       IGNORE\n",
      "reverse         0          O\n",
      "##s             -100       IGNORE\n",
      "the             0          O\n",
      "anti            0          O\n",
      "##hy            -100       IGNORE\n",
      "##pert          -100       IGNORE\n",
      "##ens           -100       IGNORE\n",
      "##ive           -100       IGNORE\n",
      "effect          0          O\n",
      "of              0          O\n",
      "c               1          B-Chemical\n",
      "##lon           -100       IGNORE\n",
      "##id            -100       IGNORE\n",
      "##ine           -100       IGNORE\n",
      ".               0          O\n",
      "[SEP]           -100       IGNORE\n"
     ]
    }
   ],
   "source": [
    "# Kita ambil sampel index ke-0\n",
    "index = 0\n",
    "input_ids = tokenized_train[\"input_ids\"][index]\n",
    "labels = tokenized_train[\"labels\"][index]\n",
    "\n",
    "print(\"Original Tokens:\", train_data[index][\"tokens\"])\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Kita loop untuk melihat pasangan Token (Kata) dengan Label ID-nya\n",
    "print(f\"{'TOKEN':<15} {'LABEL ID':<10} {'ARTI LABEL'}\")\n",
    "for id_token, id_label in zip(input_ids, labels):\n",
    "    # Abaikan padding (id 0) agar tampilan tidak kepanjangan\n",
    "    if id_token == 0: continue \n",
    "    \n",
    "    token_str = tokenizer.decode([id_token])\n",
    "    \n",
    "    # Terjemahkan label\n",
    "    if id_label == -100:\n",
    "        label_str = \"IGNORE\"\n",
    "    else:\n",
    "        label_str = id2label[id_label]\n",
    "        \n",
    "    print(f\"{token_str:<15} {id_label:<10} {label_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tahap 3: Pelatihan Model (Fine-Tuning)**\n",
    "Kita akan mengubah data hasil tokenisasi menjadi format yang bisa diterima oleh PyTorch (`Dataset` dan `DataLoader`), memuat model BERT, dan menjalankan loop pelatihan.\n",
    "\n",
    "Komponen Utama:\n",
    "1.  **NERDataset:** Wadah pembungkus data.\n",
    "2.  **DataLoader:** Pengirim data secara *batch* (paket kecil) ke model agar RAM tidak meledak.\n",
    "3.  **Model:** `BertForTokenClassification`.\n",
    "4.  **Optimizer:** `AdamW` (Algoritma untuk mengupdate otak model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memuat Model & Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Memuat model bert-base-cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model siap dilatih!\n"
     ]
    }
   ],
   "source": [
    "# --- Siapkan Model & Optimizer ---\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from torch.optim import AdamW # Ambil dari torch biar tidak error\n",
    "\n",
    "# 1. Load Model\n",
    "print(f\"‚è≥ Memuat model {MODEL_CHECKPOINT}...\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT, \n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 2. Pindah ke GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 3. Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "print(\"‚úÖ Model siap dilatih!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Memulai Training Mode 'HARDCORE' (10 Epochs)...\n",
      "   Strategi: Belajar pelan (Low LR) tapi lama (High Epochs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 654/654 [01:18<00:00,  8.37it/s, loss=0.106]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selesai Epoch 1. Rata-rata Loss: 0.1326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 654/654 [01:24<00:00,  7.73it/s, loss=0.00552] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selesai Epoch 2. Rata-rata Loss: 0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 654/654 [01:22<00:00,  7.97it/s, loss=0.00329] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selesai Epoch 3. Rata-rata Loss: 0.0240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 654/654 [01:23<00:00,  7.86it/s, loss=0.000982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selesai Epoch 4. Rata-rata Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 654/654 [01:23<00:00,  7.84it/s, loss=0.000721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selesai Epoch 5. Rata-rata Loss: 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 654/654 [01:23<00:00,  7.81it/s, loss=0.00118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selesai Epoch 6. Rata-rata Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 654/654 [01:22<00:00,  7.93it/s, loss=0.000152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selesai Epoch 7. Rata-rata Loss: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 654/654 [01:21<00:00,  8.00it/s, loss=8.57e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selesai Epoch 8. Rata-rata Loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 654/654 [01:21<00:00,  7.98it/s, loss=2.72e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selesai Epoch 9. Rata-rata Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 654/654 [01:19<00:00,  8.26it/s, loss=0.000107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selesai Epoch 10. Rata-rata Loss: 0.0005\n",
      "\n",
      "üéâ Training Maksimal Selesai! Otak model sekarang sudah 'diperas' habis-habisan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- ULTIMATE TRAINING (MAXIMIZED PERFORMANCE) ---\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# KONFIGURASI MAKSIMAL\n",
    "EPOCHS = 10           # Naik drastis supaya hafal mati\n",
    "LEARNING_RATE = 2e-5  # Lebih kecil = Lebih teliti (Precision)\n",
    "BATCH_SIZE = 8        # Tetap 8 agar aman di memori\n",
    "\n",
    "print(f\"üî• Memulai Training Mode 'HARDCORE' ({EPOCHS} Epochs)...\")\n",
    "print(\"   Strategi: Belajar pelan (Low LR) tapi lama (High Epochs)\")\n",
    "\n",
    "# Reset Model & Optimizer (Penting! Kita mulai dari nol lagi biar bersih)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT, \n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Scheduler: Mengatur kecepatan belajar (Mulai pelan, ngebut, lalu pelan lagi di akhir)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loop Training\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping (Mencegah error meledak saat belajar terlalu keras)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Selesai Epoch {epoch+1}. Rata-rata Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nüéâ Training Maksimal Selesai! Otak model sekarang sudah 'diperas' habis-habisan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tahap 4: Evaluasi dan Uji Coba (Inference)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluasi Model pada Data Validasi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Menghitung Rapot Akhir (Maximized)...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Chemical       0.92      0.92      0.92      5325\n",
      "     Disease       0.80      0.84      0.82      4223\n",
      "\n",
      "   micro avg       0.86      0.88      0.87      9548\n",
      "   macro avg       0.86      0.88      0.87      9548\n",
      "weighted avg       0.87      0.88      0.87      9548\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluasi Akurasi Pasca-Optimasi ---\n",
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "print(\"üìä Menghitung Rapot Akhir (Maximized)...\")\n",
    "model.eval()\n",
    "\n",
    "pred_list, label_list = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            temp_pred = [id2label[p] for p, l in zip(predictions[i], labels[i]) if l != -100]\n",
    "            temp_label = [id2label[l] for l in labels[i] if l != -100]\n",
    "            pred_list.append(temp_pred)\n",
    "            label_list.append(temp_label)\n",
    "\n",
    "print(classification_report(label_list, pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Manual dengan Kalimat Sendiri**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Input: The patient was prescribed Aspirin and Metformin for his chronic heart failure.\n",
      "------------------------------------------------------------\n",
      "ENTITAS (OBAT/PENYAKIT)        | KATEGORI        | YAKIN?\n",
      "------------------------------------------------------------\n",
      "üíé Aspirin                        | Chemical        | 84.6%\n",
      "üíé Metformin                      | Chemical        | 100.0%\n",
      "üíé chronic heart failure          | Disease         | 95.2%\n",
      "\n",
      "üìù Input: Long-term use of Ibuprofen can lead to kidney damage.\n",
      "------------------------------------------------------------\n",
      "ENTITAS (OBAT/PENYAKIT)        | KATEGORI        | YAKIN?\n",
      "------------------------------------------------------------\n",
      "üíé I                              | Chemical        | 100.0%\n",
      "üíé ##buprofen                     | Chemical        | 96.3%\n",
      "üíé kidney damage                  | Disease         | 100.0%\n"
     ]
    }
   ],
   "source": [
    "# --- Tes Manual PRO (Tampilan Bersih) ---\n",
    "from transformers import pipeline\n",
    "\n",
    "# Pipeline dengan strategi penggabungan (Aggregation)\n",
    "# Ini akan otomatis menyatukan \"Met\" + \"##for\" + \"##min\" -> \"Metformin\"\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    aggregation_strategy=\"simple\", # INI KUNCINYA\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def tes_canggih(kalimat):\n",
    "    print(f\"\\nüìù Input: {kalimat}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'ENTITAS (OBAT/PENYAKIT)':<30} | {'KATEGORI':<15} | {'YAKIN?'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    hasil = ner_pipeline(kalimat)\n",
    "    for h in hasil:\n",
    "        # Hanya tampilkan jika yakin di atas 50%\n",
    "        if h['score'] > 0.5:\n",
    "            print(f\"üíé {h['word']:<30} | {h['entity_group']:<15} | {h['score']:.1%}\")\n",
    "\n",
    "# UJI COBA\n",
    "tes_canggih(\"The patient was prescribed Aspirin and Metformin for his chronic heart failure.\")\n",
    "tes_canggih(\"Long-term use of Ibuprofen can lead to kidney damage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simpan Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model tersimpan aman di folder: ./model_medis_final_87persen\n",
      "Siap untuk dibandingkan dengan Model Ringan!\n"
     ]
    }
   ],
   "source": [
    "# --- SIMPAN MODEL FINAL ---\n",
    "output_dir = \"./model_medis_final_87persen\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"‚úÖ Model tersimpan aman di folder: {output_dir}\")\n",
    "print(\"Siap untuk dibandingkan dengan Model Ringan!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
